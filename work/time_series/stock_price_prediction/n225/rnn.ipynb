{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/kizawamasakazu/projects/machine_learning/work/time_series/stock_price_prediction/n225/utils.py'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "from pylab import plot\n",
    "\n",
    "import utils; reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N225株価データ\n",
    "df = pd.read_csv('N225.csv')\n",
    "df = df.set_index('Date')\n",
    "df['y'] = np.log(df['Adj Close']).diff().shift(-1)\n",
    "columns = ['Adj Close', 'High', 'Low', 'Open', 'Volume']\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        assert len(X) == len(y)\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx) -> np.ndarray:\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def make_dataset(df, seqlen):\n",
    "    X = df[['Adj Close', 'High', 'Low', 'Open']].values\n",
    "    y = df['y'].values\n",
    "    \n",
    "    datalen = X.shape[0]\n",
    "    Xs, ys = [], []  \n",
    "    for i in range(datalen-seqlen+1):\n",
    "        Xs.append(X[i:i+seqlen,:])\n",
    "        ys.append(y[i:i+1])\n",
    "    Xs = np.stack(Xs)\n",
    "    ys = np.stack(ys)\n",
    "    return MyDataset(Xs, ys)\n",
    "\n",
    "# scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df = pd.DataFrame(index=df.index)\n",
    "scaled_df[[*columns, 'y']] = scaler.fit_transform(df.loc[:, [*columns, 'y']])\n",
    "\n",
    "train, test = train_test_split(scaled_df, test_size=0.2, shuffle=False)\n",
    "train, val = train_test_split(scaled_df, test_size=0.2, shuffle=False)\n",
    "train_dataset = make_dataset(train, seqlen=60)\n",
    "val_dataset = make_dataset(val, seqlen=60)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=129)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1)\n",
    "X, y = next(iter(train_dataloader))\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.RNN(input_dim, hidden_dim, nonlinearity='tanh', batch_first=True)\n",
    "        self.l2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.l1.weight_ih_l0)\n",
    "        nn.init.orthogonal(self.l1.weight_hh_l0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h, _ = self.l1(x)\n",
    "        y = self.l2(h[:, -1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/k4676nzx20ndpwkjt51ctv380000gn/T/ipykernel_79780/4220431068.py:15: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  nn.init.orthogonal(self.l1.weight_hh_l0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.04551415020538555, val loss: 0.04251805328777138\n",
      "epoch: 1, train loss: 0.03129968176094385, val loss: 0.03185186869871449\n",
      "epoch: 2, train loss: 0.030121333098837307, val loss: 0.03151913540254177\n",
      "epoch: 3, train loss: 0.03057419862802867, val loss: 0.03463617933677955\n",
      "epoch: 4, train loss: 0.03016514228759231, val loss: 0.03285168133361239\n",
      "epoch: 5, train loss: 0.0296534845748773, val loss: 0.03398980784793974\n",
      "epoch: 6, train loss: 0.030677377010931026, val loss: 0.03883742202125805\n",
      "epoch: 7, train loss: 0.029747689797819315, val loss: 0.036147157438624074\n",
      "epoch: 8, train loss: 0.030495204461308625, val loss: 0.03228093672176482\n",
      "epoch: 9, train loss: 0.029928806821232314, val loss: 0.031631492466573985\n",
      "epoch: 10, train loss: 0.029619674310906903, val loss: 0.031725456901419334\n",
      "epoch: 11, train loss: 0.02946103953725689, val loss: 0.03719429072569794\n",
      "epoch: 12, train loss: 0.030180084815883374, val loss: 0.031572388290939196\n",
      "epoch: 13, train loss: 0.029657242553574697, val loss: 0.03348084271583759\n",
      "epoch: 14, train loss: 0.029356090965998043, val loss: 0.03403177570918916\n",
      "epoch: 15, train loss: 0.029341368762018916, val loss: 0.032641314022557835\n",
      "epoch: 16, train loss: 0.029163808817719364, val loss: 0.032426169609100046\n",
      "epoch: 17, train loss: 0.02986539623976409, val loss: 0.03149066167608113\n",
      "epoch: 18, train loss: 0.02992329402611806, val loss: 0.03315152745221702\n",
      "epoch: 19, train loss: 0.029386360259665237, val loss: 0.03247819287886082\n",
      "epoch: 20, train loss: 0.029810932168102527, val loss: 0.03143383857439941\n",
      "epoch: 21, train loss: 0.02918245770282798, val loss: 0.033633839737781335\n",
      "epoch: 22, train loss: 0.03000827442731831, val loss: 0.03148619676349868\n",
      "epoch: 23, train loss: 0.12263505877210544, val loss: 0.09732596205783561\n",
      "epoch: 24, train loss: 0.04652322646598894, val loss: 0.03310289184602214\n",
      "epoch: 25, train loss: 0.030395045201038265, val loss: 0.034745159503859535\n",
      "epoch: 26, train loss: 0.03171522029071719, val loss: 0.04065472314685163\n",
      "epoch: 27, train loss: 0.031227983677616485, val loss: 0.03142889634194508\n",
      "epoch: 28, train loss: 0.03065696524476612, val loss: 0.036995484050310834\n",
      "epoch: 29, train loss: 0.03185780362768487, val loss: 0.03645192026882105\n",
      "epoch: 30, train loss: 0.03247173024075372, val loss: 0.03717424914660588\n",
      "epoch: 31, train loss: 0.03087988600216724, val loss: 0.036672709179176415\n",
      "epoch: 32, train loss: 0.031075358718306155, val loss: 0.032916662149446115\n",
      "epoch: 33, train loss: 0.03069426624902657, val loss: 0.03164529669242845\n",
      "epoch: 34, train loss: 0.029722989158152222, val loss: 0.0319457078378805\n",
      "epoch: 35, train loss: 0.031298123668510834, val loss: 0.044088908599715836\n",
      "epoch: 36, train loss: 0.03223512160008425, val loss: 0.03149780189277421\n",
      "epoch: 37, train loss: 0.029949671906101833, val loss: 0.033739829304772366\n",
      "epoch: 38, train loss: 0.030880114411587244, val loss: 0.03612000966365908\n",
      "epoch: 39, train loss: 0.03158209673487223, val loss: 0.031454785220639804\n",
      "epoch: 40, train loss: 0.030174180906225038, val loss: 0.03165995687456198\n",
      "epoch: 41, train loss: 0.030499088670034986, val loss: 0.033237935655133825\n",
      "epoch: 42, train loss: 0.03001730320054096, val loss: 0.03146023329504779\n",
      "epoch: 43, train loss: 0.030300476393856846, val loss: 0.033054941630279513\n",
      "epoch: 44, train loss: 0.03173051903454157, val loss: 0.03169106819050413\n",
      "epoch: 45, train loss: 0.030966395594097754, val loss: 0.03796095626664833\n",
      "epoch: 46, train loss: 0.03155897966084572, val loss: 0.04141384895208856\n",
      "epoch: 47, train loss: 0.03173206326971342, val loss: 0.04143021191719552\n",
      "epoch: 48, train loss: 0.0312657955378949, val loss: 0.03248658651407336\n",
      "epoch: 49, train loss: 0.030480231946477525, val loss: 0.032964807612375475\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "input_dim = 4\n",
    "hidden_dim = 64\n",
    "model = RNN(input_dim, hidden_dim)\n",
    "optimizer = optimizers.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "def compute_loss(y, yhat):\n",
    "    return criterion(yhat, y)\n",
    "\n",
    "def train_step(x, y):\n",
    "    model.train()\n",
    "    yhat = model(x)\n",
    "    loss = compute_loss(y, yhat)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return yhat, loss\n",
    "\n",
    "def val_step(x, y):\n",
    "    model.eval()\n",
    "    yhat = model(x)\n",
    "    loss = compute_loss(y, yhat)\n",
    "    return yhat, loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = .0\n",
    "    val_loss = .0\n",
    "    \n",
    "    for X, y in train_dataloader:\n",
    "        _, loss = train_step(X, y)    \n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    for X, y in val_dataloader:\n",
    "        _, loss = val_step(X, y)\n",
    "        val_loss += loss.item()\n",
    "    val_loss /= len(val_dataset)\n",
    "    \n",
    "    print(\n",
    "        'epoch: {}, train loss: {}, val loss: {}'\\\n",
    "        .format(epoch, train_loss, val_loss)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
